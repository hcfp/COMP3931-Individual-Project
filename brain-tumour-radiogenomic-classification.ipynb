{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nimport re\nimport sys\n#!pip install git+https://github.com/shijianjian/EfficientNet-PyTorch-3D\n#!pip install torch-summary\nimport cv2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-18T15:45:15.601997Z","iopub.execute_input":"2022-04-18T15:45:15.602379Z","iopub.status.idle":"2022-04-18T15:45:15.609076Z","shell.execute_reply.started":"2022-04-18T15:45:15.602318Z","shell.execute_reply":"2022-04-18T15:45:15.608111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Clear files from previous runs in Kaggle's working environment\nfiles = glob.glob('/kaggle/working/*')\nfor f in files:\n    os.remove(f)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:15.627503Z","iopub.execute_input":"2022-04-18T15:45:15.627975Z","iopub.status.idle":"2022-04-18T15:45:15.632057Z","shell.execute_reply.started":"2022-04-18T15:45:15.627940Z","shell.execute_reply":"2022-04-18T15:45:15.631415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append(\"../input/efficientnetpyttorch3d/EfficientNet-PyTorch-3D\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:15.649861Z","iopub.execute_input":"2022-04-18T15:45:15.650318Z","iopub.status.idle":"2022-04-18T15:45:15.657752Z","shell.execute_reply.started":"2022-04-18T15:45:15.650284Z","shell.execute_reply":"2022-04-18T15:45:15.656427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Organisers have said these are to be ignored\nexclude = [109, 123, 709]\n#Load train labels from CSV and remove the exclusions\ntrain_csv = pd.read_csv('/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv')\ntrain_csv = train_csv[~train_csv.BraTS21ID.isin(exclude)]","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:15.675353Z","iopub.execute_input":"2022-04-18T15:45:15.675748Z","iopub.status.idle":"2022-04-18T15:45:15.698773Z","shell.execute_reply.started":"2022-04-18T15:45:15.675716Z","shell.execute_reply":"2022-04-18T15:45:15.698149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the number of each class\nplt.figure(figsize=(7, 7))\nsns.countplot(data=train_csv, x=\"MGMT_value\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:15.702140Z","iopub.execute_input":"2022-04-18T15:45:15.704720Z","iopub.status.idle":"2022-04-18T15:45:15.938281Z","shell.execute_reply.started":"2022-04-18T15:45:15.704682Z","shell.execute_reply":"2022-04-18T15:45:15.937596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The number of images in a 3D volume\nNUM_IMAGES = 64\nmri_types = ['FLAIR','T1w','T1wCE','T2w']\n#The resolution for all 2D images SIZExSIZE\nSIZE = 224\n\n#https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n#Loads a single slice from a DICOM file\ndef load_dicom(path, voi_lut = True, fix_monochrome = True, augment=False):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # MONOCHROME1 means that the brightness decreases as the value increases\n    # This would lead to inverted images, so we reverse this\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    \n    #Required in training only\n    if augment:\n        rotation_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n        choice = np.random.randint(0,4)\n        rotation = rotation_choices[choice]\n        data = cv2.rotate(data, rotation)\n    #Correct to size required by model and change data type\n    data = cv2.resize(data, (SIZE, SIZE))\n    data = data.astype(np.float64)\n    return data\n\n#loads 64 (SIZE) images from a DICOM file from a single MRI type\ndef load_dicom_3d(brats21id, mri_type, num_imgs=NUM_IMAGES, img_size=SIZE, data_split=\"train\", augment=False):\n    #Create list of all paths to the specified MRI type images for a specified patient\n    patient_path = os.path.join(f\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/{data_split}/\", str(brats21id).zfill(5))\n    x = glob.glob(os.path.join(patient_path, mri_type, \"*\"))\n    paths = sorted(x, key=lambda x: int(x[:-4].split(\"-\")[-1]))\n    \n    #Used to get correct subset of slices\n    middle = len(paths) // 2\n    num_imgs2 = num_imgs // 2\n    bottom = max(0, middle - num_imgs2)\n    top = min(len(paths), middle + num_imgs2)\n    \n    image_3d = np.stack([load_dicom(f, augment=augment) for f in paths[bottom:top]]).transpose()\n    \n    #Add padding if there are less than 64 images\n    if image_3d.shape[-1] < num_imgs:\n        zero_padding = np.zeros((img_size, img_size, abs(num_imgs - image_3d.shape[-1])))\n        image_3d = np.concatenate((image_3d, zero_padding), axis=-1)\n    \n    #normalise data\n    if np.min(image_3d) < np.max(image_3d):\n        image_3d = image_3d - np.min(image_3d)\n        image_3d = image_3d / np.max(image_3d)\n    return np.expand_dims(image_3d, 0)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:15.942639Z","iopub.execute_input":"2022-04-18T15:45:15.944552Z","iopub.status.idle":"2022-04-18T15:45:15.966401Z","shell.execute_reply.started":"2022-04-18T15:45:15.944512Z","shell.execute_reply":"2022-04-18T15:45:15.965698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Testing it works\na = load_dicom_3d(9, \"T1wCE\")\nprint(a.shape)\nprint(np.min(a), np.max(a), np.mean(a), np.median(a))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:15.967582Z","iopub.execute_input":"2022-04-18T15:45:15.968173Z","iopub.status.idle":"2022-04-18T15:45:16.982814Z","shell.execute_reply.started":"2022-04-18T15:45:15.968132Z","shell.execute_reply":"2022-04-18T15:45:16.981906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displays the middle sample of each MRI type for an ID\ndef visualise_sample(brats21id, mgmt_value, types=(\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\")):\n    plt.figure(figsize=(16, 5)) \n    patient_path = os.path.join(\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/train/\", str(brats21id).zfill(5))\n    for i, type in enumerate(types, 1):\n        x = glob.glob(os.path.join(patient_path, type, \"*\"))\n        type_paths = sorted(glob.glob(os.path.join(patient_path, type, \"*\")), key=lambda x: int(x[:-4].split(\"-\")[-1]))\n        data = load_dicom(type_paths[(len(type_paths) // 2)])\n        plt.subplot(1, 4, i)\n        plt.imshow(data, cmap=\"gray\")\n        plt.title(f\"{type}\", fontsize=16)\n\n    plt.suptitle(f\"BraTS21ID: {brats21id}  MGMT_value: {mgmt_value}\", fontsize=16)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:16.987693Z","iopub.execute_input":"2022-04-18T15:45:16.988431Z","iopub.status.idle":"2022-04-18T15:45:17.000486Z","shell.execute_reply.started":"2022-04-18T15:45:16.988393Z","shell.execute_reply":"2022-04-18T15:45:16.999772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Displays a random patient\nrandom_sample = train_csv.sample(n=1).values\nrandom_sample_ID = random_sample[0][0]\nrandom_sample_MGMT = random_sample[0][1]\nvisualise_sample(1002, random_sample_MGMT)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:17.004979Z","iopub.execute_input":"2022-04-18T15:45:17.007811Z","iopub.status.idle":"2022-04-18T15:45:17.656951Z","shell.execute_reply.started":"2022-04-18T15:45:17.007708Z","shell.execute_reply":"2022-04-18T15:45:17.656236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from efficientnet_pytorch_3d import EfficientNet3D\nfrom efficientnet_pytorch_3d.utils import Conv3dStaticSamePadding\n\nimport torch\nfrom torch import nn\ntorch.set_default_dtype(torch.float64)\n\n#dataset splitting\nfrom sklearn import model_selection as sk_model_selection\n\n#convolutional\nfrom torch.nn import functional as torch_functional","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:17.658283Z","iopub.execute_input":"2022-04-18T15:45:17.658556Z","iopub.status.idle":"2022-04-18T15:45:17.665316Z","shell.execute_reply.started":"2022-04-18T15:45:17.658521Z","shell.execute_reply":"2022-04-18T15:45:17.664656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split datasets\ntrain, valid = sk_model_selection.train_test_split(train_csv, random_state=3220, stratify=train_csv[\"MGMT_value\"],)\nvalid","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:17.666470Z","iopub.execute_input":"2022-04-18T15:45:17.666761Z","iopub.status.idle":"2022-04-18T15:45:17.683687Z","shell.execute_reply.started":"2022-04-18T15:45:17.666725Z","shell.execute_reply":"2022-04-18T15:45:17.682991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, image_IDs, targets=None, mri_type=None, split=\"train\", augment=False):\n        self.image_IDs = image_IDs\n        self.targets = targets\n        self.mri_type = mri_type\n        self.split = split\n        self.augment = augment\n        \n    def __len__(self):\n        return len(self.image_IDs)\n    \n    def __getitem__(self, i: int):\n        image_num = self.image_IDs[i]\n        #targets is None when testing\n        if self.targets is not None:\n            data = load_dicom_3d(image_num, self.mri_type, augment=self.augment)\n        else:\n            data = load_dicom_3d(image_num, self.mri_type, data_split=self.split, augment=self.augment)\n        \n        #The -0.01 is label smoothing\n        if self.targets is not None:\n            return {\"X\": data, \"y\": torch.tensor(abs(self.targets[i]-0.01), dtype=torch.float64)}\n        else:\n            return {\"X\": torch.tensor(data, dtype=torch.float64), \"id\": image_num}","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:17.685300Z","iopub.execute_input":"2022-04-18T15:45:17.685747Z","iopub.status.idle":"2022-04-18T15:45:17.694916Z","shell.execute_reply.started":"2022-04-18T15:45:17.685712Z","shell.execute_reply":"2022-04-18T15:45:17.694202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        #create a network using efficientnet-b0, that has 2 output classes (MGMT 1/0) and 1 input channel as it is greyscale\n        self.network = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\n        #replace the 2 outputs with a linear classifiers - essentially gives the probabilty\n        #This is the format required for submission\n        self.network._fc = nn.Linear(in_features=self.network._fc.in_features, out_features=1, bias=True)\n        \n    def forward(self, x):\n        out = self.network(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:17.697110Z","iopub.execute_input":"2022-04-18T15:45:17.697328Z","iopub.status.idle":"2022-04-18T15:45:17.707656Z","shell.execute_reply.started":"2022-04-18T15:45:17.697304Z","shell.execute_reply":"2022-04-18T15:45:17.706876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import roc_auc_score\nclass Trainer:\n    def __init__(self, model, device, optimizer, criterion):\n        self.model = model\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.output_model = None\n        self.best_auc = 0\n        \n    def fit(self, epochs, train_loader, valid_loader, path):\n        all_training_losses = []\n        all_valid_losses = []\n        all_auc = []\n        for epoch_n in range(1, epochs + 1):\n            print(f\"Epoch {epoch_n}\")\n            train_loss = self.train_one_epoch(train_loader)\n            valid_loss, auc = self.validate_one_epoch(valid_loader)\n            all_training_losses.append(train_loss)\n            all_valid_losses.append(valid_loss)\n            all_auc.append(auc)\n            print(\"Training loss: \", train_loss)\n            print(\"Validation loss: \", valid_loss)\n            if auc > self.best_auc:\n                self.save_model(path, valid_loss, auc)\n                self.best_auc = auc\n        return all_training_losses, all_valid_losses, all_auc\n                    \n    def train_one_epoch(self, train_dataloader):\n        sum_losses = 0\n        self.model.train()\n        time_start = time.time()\n        for step, batch in enumerate(train_dataloader, 1):\n            print(\"Step: \", step, end='\\r')\n            ids = batch[\"X\"].to(self.device)\n            targets = batch[\"y\"].to(self.device)\n            \n            #pass inputs through model\n            self.optimizer.zero_grad()\n            outputs = self.model(ids).squeeze(1)\n            \n            #Calculate loss and optimise/backpropogate\n            loss = self.criterion(outputs, targets)\n            loss.backward()\n\n            sum_losses += loss.detach().item()\n\n            self.optimizer.step()\n        print(time.time() - time_start)\n        average_loss = sum_losses/len(train_dataloader)\n        return average_loss\n\n    \n    def validate_one_epoch(self, valid_dataloader):\n        #set model up for evaluation, disables dropout layers etc.\n        self.model.eval()\n        sum_losses = 0\n        all_outputs = []\n        all_targets = []\n        time_start = time.time()\n        for step, batch in enumerate(valid_dataloader, 1):\n            #deactivate autograd\n            with torch.no_grad():\n                X = batch[\"X\"].to(self.device)\n                targets = batch[\"y\"].to(self.device)\n                \n                outputs = self.model(X).squeeze(1)\n                loss = self.criterion(outputs, targets)\n                \n                sum_losses += loss.detach().item()\n                all_targets.extend(batch[\"y\"].tolist())\n                all_outputs.extend(torch.sigmoid(outputs).tolist())\n                \n        #Make targets discrete as required for AUC calculation\n        #Were previously made continuous during label smoothing\n        all_targets = [1 if x > 0.5 else 0 for x in all_targets]\n        \n        auc = roc_auc_score(all_targets, all_outputs)\n        print(\"Time: \", time.time() - time_start, \"AUC: \", auc)\n        average_loss = sum_losses/len(valid_dataloader) \n        return average_loss, auc\n\n    def save_model(self, path, loss, auc):\n        self.output_model = f\"{path}_{loss:.2f}_{auc:.2f}.pt\"\n        print(f\"Saving {self.output_model}\")\n        torch.save(self.model.state_dict(), self.output_model)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:17.711090Z","iopub.execute_input":"2022-04-18T15:45:17.711352Z","iopub.status.idle":"2022-04-18T15:45:17.729627Z","shell.execute_reply.started":"2022-04-18T15:45:17.711318Z","shell.execute_reply":"2022-04-18T15:45:17.728660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from torchsummary import summary\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef train_model(train, valid, mri_type, epochs):\n    train_dataset = Dataset(\n        train[\"BraTS21ID\"].values,\n        train[\"MGMT_value\"].values,\n        mri_type=mri_type,\n        split=\"train\",\n        augment=True\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=4,\n            shuffle=True,\n            num_workers=2,\n            pin_memory = True\n    )\n    \n    valid_dataset = Dataset(\n        valid[\"BraTS21ID\"].values,\n        valid[\"MGMT_value\"].values,\n        mri_type=mri_type,\n        split=\"train\"\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n            valid_dataset,\n            batch_size=4,\n            shuffle=True,\n            num_workers=2,\n            pin_memory = True\n    )\n    \n    model = Model()\n    model.to(device)\n    #summary(model, input_size=(1, 256, 256, 64))\n    optimiser = torch.optim.AdamW(model.parameters(), lr=0.001)\n    criterion = torch_functional.binary_cross_entropy_with_logits\n\n    trainer = Trainer(model, device, optimiser, criterion)\n    history = trainer.fit(epochs, train_loader, valid_loader, mri_type)\n    train_losses = history[0]\n    valid_losses = history[1]\n    aucs = history[2]\n    \n    return trainer.output_model, aucs, train_losses, valid_losses","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:17.730879Z","iopub.execute_input":"2022-04-18T15:45:17.731748Z","iopub.status.idle":"2022-04-18T15:45:17.743513Z","shell.execute_reply.started":"2022-04-18T15:45:17.731695Z","shell.execute_reply":"2022-04-18T15:45:17.742678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_paths = []\nepochs = 15\nepochs_list = list(range(1, epochs + 1))\n\n#Train a new model for each MRI type and collect the paths for the best model from each\nfor train_type in mri_types:\n    trained_model_path, aucs, train_losses, valid_losses = train_model(train, valid, train_type, epochs)\n    trained_model_paths.append(trained_model_path)\n    \n    #Make training visualisation\n    fig, axs = plt.subplots(1, 3)\n    fig.suptitle(f\"AUC and loss values over {epochs} epochs ({train_type})\")\n    fig.tight_layout()\n    axs[0].plot(epochs_list, aucs)\n    axs[0].set_title(\"AUC\")\n    axs[1].plot(epochs_list, train_losses)\n    axs[1].set_title(\"Training loss\")\n    axs[2].plot(epochs_list, valid_losses)\n    axs[2].set_title(\"Validation loss\")\n\nprint(trained_model_paths)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:45:17.746224Z","iopub.execute_input":"2022-04-18T15:45:17.746419Z","iopub.status.idle":"2022-04-18T15:53:45.593691Z","shell.execute_reply.started":"2022-04-18T15:45:17.746396Z","shell.execute_reply":"2022-04-18T15:53:45.591240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_prediction(path, valid, mri_type, split):\n    print(f\"Predicting with {path}\")\n    \n    valid.loc[:,\"MRI_Type\"] = mri_type\n    dataset = Dataset(\n        valid.index.values,\n        mri_type=mri_type,\n        split=split\n    )\n    \n    dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=4,\n            shuffle=False,\n            num_workers=2\n    )\n    \n    model = Model()\n    model.to(device)\n    #Load a previously trained model\n    model.load_state_dict(torch.load(path))\n    #Disables dropout/pooling for inference\n    model.eval()\n    \n    MGMT_prediction = []\n    patient_ids = []\n    \n    #Do not calculate gradients\n    for step, batch in enumerate(dataloader, 1):\n        with torch.no_grad():\n            #Send batch X through model and make classification with sigmoid\n            prediction_sigmoid = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n            #Make list of predicitons\n            if prediction_sigmoid.size != 1:\n                MGMT_prediction.extend(prediction_sigmoid.tolist())\n            else:\n                #if batch size happens to be 1\n                MGMT_prediction.append(prediction_sigmoid)\n            patient_ids.extend(batch[\"id\"].numpy().tolist())\n    #Convert to DataFrame/CSV\n    prediction = pd.DataFrame({\"BraTS21ID\": patient_ids, \"MGMT_value\": MGMT_prediction}) \n    prediction = prediction.set_index(\"BraTS21ID\")\n    return prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid.set_index(\"BraTS21ID\", inplace=True)\nvalid.loc[:, \"MGMT_prediction\"] = 0.0\n\nfor path in trained_model_paths:\n    mri_type = path.split(\"_\")[0]\n    #make predictions with each of the best models\n    prediction = make_prediction(path, valid, mri_type=mri_type, split=\"train\")\n    valid.loc[:,\"MGMT_prediction\"] += prediction.loc[:,\"MGMT_value\"]\n#Average out predictions\nvalid.loc[:,\"MGMT_prediction\"] /= 4\n\nauc = roc_auc_score(valid.loc[:, \"MGMT_value\"], valid.loc[:, \"MGMT_prediction\"])\nprint(f\"Validation AUC: {auc}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:53:45.597051Z","iopub.status.idle":"2022-04-18T15:53:45.597639Z","shell.execute_reply.started":"2022-04-18T15:53:45.597401Z","shell.execute_reply":"2022-04-18T15:53:45.597428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot distribution of predictions\nsns.displot(valid.loc[:, \"MGMT_prediction\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:53:45.598724Z","iopub.status.idle":"2022-04-18T15:53:45.599292Z","shell.execute_reply.started":"2022-04-18T15:53:45.599042Z","shell.execute_reply":"2022-04-18T15:53:45.599069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(f\"/kaggle/input/rsna-miccai-brain-tumor-radiogenomic-classification/sample_submission.csv\", index_col=\"BraTS21ID\")\n\nsubmission[\"MGMT_value\"] = 0.0\n\n#Make predicitons on test dataset for submission\nfor path in trained_model_paths:\n    mri_type = path.split(\"_\")[0]\n    prediction = make_prediction(path, submission, train_type, split=\"test\")\nsubmission[\"MGMT_value\"] += prediction[\"MGMT_value\"]\nsubmission.loc[:,\"MGMT_value\"] /= 4\nsubmission[\"MGMT_value\"].to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:53:45.600431Z","iopub.status.idle":"2022-04-18T15:53:45.600981Z","shell.execute_reply.started":"2022-04-18T15:53:45.600742Z","shell.execute_reply":"2022-04-18T15:53:45.600768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(submission[\"MGMT_value\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:53:45.602074Z","iopub.status.idle":"2022-04-18T15:53:45.602665Z","shell.execute_reply.started":"2022-04-18T15:53:45.602422Z","shell.execute_reply":"2022-04-18T15:53:45.602448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualisation","metadata":{}},{"cell_type":"code","source":"#Load a previously trained model\nmodel = Model()\nmodel.to(device)\nmodel.load_state_dict(torch.load(trained_model_path))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:53:45.603763Z","iopub.status.idle":"2022-04-18T15:53:45.604327Z","shell.execute_reply.started":"2022-04-18T15:53:45.604075Z","shell.execute_reply":"2022-04-18T15:53:45.604101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load a \"random\" patient volume\nimg_3d = load_dicom_3d(1002, \"FLAIR\")\n#convert to type required to pass in to model\nimg_3d = torch.tensor(img_3d, dtype=torch.float64).unsqueeze(0).cuda()\n\n#Get weights for convolutional layers from the trained model\nconv_weight = model.network._conv_stem.weight.cpu().detach().numpy()\nprint(conv_weight.shape)\n#Displays convolutional weights before any input\nplt.figure(figsize=(10, 10))\nfor i in range(32):\n    plt.subplot(6, 6, i+1)\n    plt.imshow(conv_weight[i, 0, :, :, 2], cmap=\"gray\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:53:45.605439Z","iopub.status.idle":"2022-04-18T15:53:45.605989Z","shell.execute_reply.started":"2022-04-18T15:53:45.605749Z","shell.execute_reply":"2022-04-18T15:53:45.605776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(img_3d.shape)\n#Shows the loaded MRI\nplt.imshow(img_3d[0, 0, :, :, 10].cpu().detach().numpy(), cmap=\"gray\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:53:45.607074Z","iopub.status.idle":"2022-04-18T15:53:45.607646Z","shell.execute_reply.started":"2022-04-18T15:53:45.607410Z","shell.execute_reply":"2022-04-18T15:53:45.607434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#List of all network components\nnet_modules = list(model.network.modules())\nconv_layers = []\nlayer_weights = []\nconv_count = 0\n\n#create lists of all Conv3dStaticSamePadding modules and their weights\nfor module in net_modules:\n    if type(module) == Conv3dStaticSamePadding:\n        conv_layers.append(module)\n        layer_weights.append(module.weight)\n        \noutput = []\nnames = []\nimage = img_3d\n\n#pass image through each layer. Previous layer output passed to next layer\nfor l in conv_layers:\n    names.append(str(l).split(\"(\")[0])\n    image = l(image)\n    output.append(image)\n\n#reduce dimensions so that it can be visualised\nflattened_output = []\nfor o in output:\n    o = o.squeeze(0)\n    grey_scale = torch.sum(o ,0)\n    grey_scale = grey_scale / o.shape[0]\n    flattened_output.append(grey_scale.cpu().detach().numpy())\n\n#Plot all conv layers after input passed\nfig = plt.figure(figsize=(30, 50))\nfor i in range(len(flattened_output)):\n    sub_plot = fig.add_subplot(9, 9, i+1)\n    image = flattened_output[i]\n    imgplot = plt.imshow(image[:, :, 0])\n    sub_plot.axis(\"off\")\n    sub_plot.set_title(names[i], fontsize=10)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-18T15:53:45.608756Z","iopub.status.idle":"2022-04-18T15:53:45.609320Z","shell.execute_reply.started":"2022-04-18T15:53:45.609071Z","shell.execute_reply":"2022-04-18T15:53:45.609095Z"},"trusted":true},"execution_count":null,"outputs":[]}]}